---
layout: post
title: "2025년의 VLM : 더 좋아지고, 더 빠르고, 더 강해진 비전 언어 모델"
author: youngjun
categories: [VLM]
image: assets/images/blog/posts/2025-10-20-2025-VLM/thumbnail.png
---
* TOC
{:toc}
<!--toc-->

_이 글은 Hugging Face 블로그의 [Vision Language Models (Better, Faster, Stronger)](https://huggingface.co/blog/vlms-2025)를 한국어로 번역한 글입니다._

---
## 들어가며

 비전 언어 모델(VLM)이 요즘 화제입니다. 이전 [블로그 포스트](https://huggingface.co/blog/vlms)(2024년 4월 포스팅)에서 우리는 VLM들에 대해 깊이 있게 다뤘습니다. 그중 상당 부분은 최초의 성공적이고 재현이 용이한 오픈소스 VLM인 LLaVA에 관한 내용이었으며, 오픈 모델을 발견하고 평가하며 미세 조정하는 방법에 대한 팁도 함께 소개했습니다.

그 이후로 많은 변화가 있었습니다. 모델들은 더 작아졌지만 더 강력해졌습니다. 새로운 아키텍처와 기능들(추론, 자율성, 긴 영상 이해 등)이 등장했습니다. 동시에 멀티모달 검색-증강 생성(Multimodal RAG)과 멀티모달 에이전트(Multimodal Agent)와 같은 완전히 새로운 패러다임이 형성되었습니다.

이 포스트에서는 지난 한 해 동안 VLM에서 일어난 모든 일들을 되돌아보고 분석해 보겠습니다. 주요 변화, 떠오르는 트렌드, 그리고 주목할 만한 발전사항들을 확인하실 수 있을 것입니다.

> 비전 언어 모델이 어떻게 작동하는지에 대한 좋은 입문서를 원하신다면 첫 번째 블로그 포스트를 읽어보시길 강력히 권장합니다. (Hugging Face KREW Blog 블로그에서도 번역했답니다!)

## 모델 살펴보기

 이 섹션에서는 새로운 유형의 VLM을 살펴보겠습니다. 일부는 완전히 새로운 반면, 다른 일부는 기존 연구의 개선된 버전입니다.

### Any-to-Any 모델
 Any-to-any 모델은 이름에서 알 수 있듯이 모든 모달리티를 입력으로 받아 모든 모달리티(이미지, 텍스트, 오디오)를 출력할 수 있는 모델입니다. 이는 모달리티간 정렬(align)을 통해 이루어지며, 한 모달리티의 입력을 다른 모달리티로 변환할 수 있습니다(예: "개"라는 단어가 개의 이미지나 해당 단어의 발화음과 연관됨).

이러한 모델들은 다중 인코더(각 모달리티당 하나)를 가지고 있으며, 임베딩을 함께 융합하여 공유 표현 공간을 만듭니다. 디코더(다중 또는 단일)는 공유 잠재 공간을 입력받아 선택한 모달리티로 디코딩합니다. 최초의 Any-to-any 모델 구축 시도는 `Meta`의 [Chameleon](https://huggingface.co/collections/facebook/chameleon-668da9663f80d483b4c61f58)으로, 이미지와 텍스트를 입력받아 이미지와 텍스트를 출력할 수 있습니다. Meta는 이 모델의 이미지 생성 기능을 공개하지 않았기 때문에, `Alpha-VLLM`이 Chameleon 위에 이미지 생성을 구축한 [Lumina-mGPT](https://huggingface.co/collections/Alpha-VLLM/lumina-mgpt-family-66ae48a59a973eeae4513848)를 출시했습니다.

최신이자 가장 강력한 any-to-any 모델인 [Qwen 2.5 Omni](https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e)(아래 그림)는 any-to-any 모델의 아키텍처를 이해하기 좋은 예입니다.

![](https://i.imgur.com/Zmerw7r.png)

Qwen2.5-Omni는 새로운 "Thinker-Talker" 아키텍처를 사용하는데, "Thinker"가 텍스트 생성을 담당하고, "Talker"가 스트리밍 방식으로 자연스러운 음성 응답을 생성합니다. [MiniCPM-o 2.6](https://huggingface.co/openbmb/MiniCPM-o-2_6)은 8B 파라미터를 가진 멀티모달 모델로, 비전, 음성, 언어 모달리티에 걸쳐 콘텐츠를 이해하고 생성할 수 있습니다. DeepSeek AI가 소개한 [Janus-Pro-7B](https://huggingface.co/deepseek-ai/Janus-Pro-7B)는 모달리티 간 콘텐츠 이해와 생성 모두에 뛰어난 통합 멀티모달 모델입니다. 이해와 생성 프로세스를 분리하는 분리된 시각적 인코딩 아키텍처가 특징입니다.

우리는 앞으로 이러한 모델의 수가 증가할 것으로 예상합니다. 멀티모달 학습이 심층 표현을 더 잘 학습할 수 있는 유일한 방법이라는 것은 잘 알려진 직관입니다. 우리는 이 [컬렉션](https://huggingface.co/collections/merve/any-to-any-models-datasets-spaces-6822042ee8eb7fb5e38f9b62)에 몇 가지 any-to-any 모델과 데모를 큐레이션했습니다.

### 추론 모델
 추론 모델은 복잡한 문제를 해결할 수 있는 모델입니다. 우리는 대규모 언어 모델에서 처음 이를 보았고, 이제 VLM에서도 볼 수 있습니다. 2025년까지는 Qwen의 [QVQ-72B-preview](https://huggingface.co/Qwen/QVQ-72B-Preview)라는 단 하나의 오픈소스 멀티모달 추론 모델만 있었습니다. 이것은 `Alibaba Qwen` 팀이 개발한 실험적 모델로 많은 제한 사항이 있었습니다.

올해에는 또 다른 주자가 등장했습니다. `Moonshot AI` 팀의 [Kimi-VL-A3B-Thinking](https://huggingface.co/moonshotai/Kimi-VL-A3B-Thinking)입니다. 이 모델은 이미지 인코더로 MoonViT(SigLIP-so-400M)를 사용하고, 총 16B 파라미터에 2.8B의 활성 파라미터만 있는 전문가 혼합(MoE) 디코더를 사용합니다. 이 모델은 Kimi-VL 기반 VLM을 긴 생각의 연쇄(long chain-of-thought) 방식으로 미세 조정하고 및 추가 정렬(reinforcement learning, 강화 학습)한 버전입니다. [여기서](https://huggingface.co/spaces/moonshotai/Kimi-VL-A3B-Thinking) 모델을 체험해볼 수 있습니다.

> 저자들은 또한 [Kimi-VL-A3B-Instruct](https://huggingface.co/moonshotai/Kimi-VL-A3B-Instruct)라는 지시문 미세 조정 버전도 출시했습니다.
 
![d3ACkiw.png](https://i.imgur.com/d3ACkiw.png)

이 모델은 긴 영상, PDF, 스크린샷 등을 입력받을 수 있습니다. 에이전트 기능도 가지고 있습니다.

### 작지만 강력한 모델
 커뮤니티는 과거에 파라미터 수를 통해 지능을 확장하고, 이후 고품질 합성 데이터를 활용하곤 했습니다. 특정 시점이 지나자 벤치마크가 포화 상태에 이르렀고, 모델 확장은 수익이 감소했습니다. 커뮤니티는 증류(distillation)와 같은 다양한 방법을 통해 대규모 모델을 축소하기 시작했습니다. 이는 컴퓨팅 비용을 절감하고 배포를 단순화하며, 로컬 실행과 같은 사용 사례를 가능하게 하여 데이터 프라이버시를 강화하기 때문에 합리적인 접근입니다.

작은 VLM이란 일반적으로 소비자용 GPU에서 실행할 수 있는 2B 파라미터 미만의 모델을 말합니다. SmolVLM은 소형 VLM의 대표적인 모델 계열입니다. 더 큰 모델을 축소하는 대신, 저자들은 대형 모델을 축소하는 방식이 아니라, **아예** 256M, 500M, 2.2B처럼 극히 적은 파라미터 규모에 모델을 맞추는 정반대의 접근을 시도했습니다. 예를 들어 SmolVLM2는 이러한 크기에서 영상 이해 문제를 해결하려고 시도했고, 500M이 적절한 균형점임을 발견했습니다. Hugging Face에서 이러한 모델 규모로도 소비자 기기에서 영상 이해가 가능함을 보여주기 위해 HuggingSnap이라는 iPhone 애플리케이션을 만들었습니다.

또 다른 주목할 만한 모델은 `Google DeepMind`의 [gemma3-4b-it](https://huggingface.co/google/gemma-3-4b-it)입니다. 128k 토큰 컨텍스트 윈도우를 가진 가장 작은 멀티모달 모델 중 하나이며, 140개 이상의 언어를 지원한다는 점에서 특히 흥미롭습니다. 이 모델은 Gemma 3 모델 패밀리의 일부로, 가장 큰 모델은 당시 Chatbot Arena에서 1위를 차지했습니다. 그 후 가장 큰 모델이 1B 변형으로 증류되었습니다.

마지막으로, 가장 작은 모델은 아니지만 [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)도 주목할 만합니다. 이 모델은 지역화(객체 감지 및 포인팅)부터 문서 이해, 에이전트 작업까지 다양한 작업을 수행할 수 있으며, 컨텍스트 길이는 최대 32k 토큰입니다.

MLX 및 Llama.cpp 통합을 통해 작은 모델을 사용할 수 있습니다. MLX의 경우, 설치되어 있다고 가정하고, 이 한 줄로 SmolVLM-500M-Instruct를 시작할 수 있습니다:
```shell
python3 -m mlx_vlm.generate --model HuggingfaceTB/SmolVLM-500M-Instruct --max-tokens 400 --temp 0.0 --image https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vlm_example.jpg --prompt "What is in this image?"
```

CLI로 llama.cpp를 통해 GGUF 형식의 [gemma-3-4b-it](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d) 모델 사용도 이 한 줄로 시작할 수 있습니다:
```shell
llama-mtmd-cli -hf ggml-org/gemma-3-4b-it-GGUF
```

또한 다음과 같이 동일한 모델을 서빙할 수 있습니다:
```shell
llama-server -hf ggml-org/gemma-3-4b-it-GGUF
```

가장 초기의 VLM 시도로 [moondream2](https://huggingface.co/vikhyatk/moondream2)와 [Florence-2](https://huggingface.co/collections/microsoft/florence-6669f44df0d87d9c3bfb76de)를 소개합니다. 이 블로그에서는 주로 새로운 모델들(대부분 2024년 4월 이후 출시된 모델)을 다룹니다.

### 디코더로서의 전문가 혼합(Mixture-of-Experts, MoE)
전문가 혼합(MoE) 모델은 주어진 입력 데이터 세그먼트를 처리하기 위해 "전문가"라고 불리는, 가장 관련성 있는 하위 모델만 동적으로 선택하고 활성화함으로써 밀집 아키텍처에 대한 대안을 제공합니다. 이 선택적 활성화(라우터에 의해 수행됨) 메커니즘은 더 적은 계산 리소스를 사용하면서도 모델 성능과 운영 효율성을 크게 향상시킬 수 있는 잠재력을 보여주었습니다.

MoE는 네트워크의 더 작은 부분만 선택적으로 활성화하기 때문에 유사한 파라미터 밀집 모델보다 추론 속도가 빠릅니다. 또한 훈련 중에 빠르게 수렴합니다. 모든 좋은 것에는 비용이 따르는데, MoE는 작은 청크만 사용되더라도 모든 모델이 GPU에 있어야 하므로 더 많은 메모리 비용이 필요합니다.

널리 채택된 Transformer 아키텍처에서 MoE 레이어는 가장 일반적으로 각 Transformer 블록 내의 표준 피드포워드 네트워크(FFN) 레이어를 대체하여 통합됩니다. 밀집 네트워크는 추론을 실행하기 위해 전체 모델을 사용하는 반면, 유사한 크기의 MoE 네트워크는 일부 '전문가'만 선택적으로 활성화합니다. 이것은 더 나은 계산 활용과 더 빠른 추론에 도움이 됩니다.

전문가 혼합 디코더를 가진 VLM은 성능이 향상된 것으로 보입니다. 예를 들어, Kimi-VL은 현재 전문가 혼합 디코더를 가진 가장 진보된 개방형 추론 모델입니다. 전문가 혼합은 [MoE-LLaVA](https://huggingface.co/papers/2401.15947)의 효율성과 환각 감소에 대한 초점, [DeepSeek-VL2](https://huggingface.co/deepseek-ai/deepseek-vl2)의 광범위한 멀티모달 기능에서도 유망한 결과를 보여줍니다. Llama의 최신 버전(Llama 4)은 비전 기능을 가진 MoE입니다. 디코더로서의 MoE는 유망한 연구 분야이며, 이와 유사한 모델의 증가가 예상됩니다.

> MoE에 대해 더 잘 이해하고 싶으면, [이 글](https://huggingface.co/blog/moe)을 읽어보시는것을 추천드립니다.

### VLA(비전-언어-행동 모델) 

VLM은 로봇 공학 분야에서도 두각을 나타내고 있습니다! 거기서는 비전-언어-행동 모델(VLA)로 알려져 있습니다. 하지만 속지 마세요, 이것들은 주로 작은 콧수염과 모자를 쓴 VLM에 불과합니다. VLA는 이미지와 텍스트 지시를 받아 로봇이 직접 취해야 할 행동을 나타내는 텍스트를 반환합니다. VLA는 물리적 환경과 상호작용하고 제어하기 위해 행동 및 상태 토큰을 추가하여 비전 언어 모델을 확장합니다. 이러한 추가 토큰은 시스템의 내부 상태(환경을 인지하는 방법), 행동(명령에 기반한 수행 내용), 시간 관련 정보(작업의 단계 순서같은)를 나타냅니다. 이러한 토큰은 행동이나 정책을 생성하기 위해 비전 언어 입력에 추가됩니다.

VLA는 보통 기본 VLM 위에 미세 조정됩니다. 일부 사람들은 이 정의를 확장하여 실제 또는 디지털 세계와 시각적으로 상호작용하는 모든 모델을 VLA로 정의하기도 합니다. 이 정의에 따르면 VLA는 UI 탐색을 수행하거나 에이전트 워크플로우에 사용될 수 있습니다. 하지만 많은 사람들은 이러한 애플리케이션이 VLM 도메인에 속한다고 믿습니다.

VLA의 좋은 예는 `Physical Intelligence`의 π0과 π0-FAST로, Hugging Face의 LeRobot 라이브러리에 이식된 최초의 로보틱스 공학 모델입니다. 이러한 모델은 7개의 로봇 플랫폼과 68개의 고유한 작업에 걸쳐 훈련되었습니다. 이들은 빨래 접기, 테이블 정리, 식료품 포장, 상자 조립, 물체 검색과 같은 복잡하고 실제적인 활동에서 강력한 제로샷 및 미세 조정 성능을 보여줍니다.

[GR00T N1](https://huggingface.co/nvidia/GR00T-N1-2B)은 NVIDIA의 범용 휴머노이드 로봇을 위한 오픈 VLA 파운데이션 모델입니다. 이미지와 언어를 이해하고, 지능적 추론과 실시간 동작 제어를 결합한 시스템 덕분에 팔을 움직이거나 지시를 따르는 등의 행동으로 변환합니다. GR00T N1은 또한 로봇 시연을 공유하고 훈련하는 것을 단순화하기 위해 만들어진 오픈 표준인 LeRobot 데이터셋 형식 위에 구축되었습니다.

![](https://i.imgur.com/9gI91Wh.png)

이제 최신 VLM 모델 혁신을 살펴보았으니, 더 확립된 기능들이 어떻게 발전했는지 탐구해봅시다.

## 특화된 기능들

### VLM을 이용한 객체 감지, 분할, 계수
앞서 살펴본 바와 같이, VLM은 전통적인 컴퓨터 비전 작업에 대한 일반화를 가능하게 합니다. 이제 모델은 이미지와 개방형 텍스트와 같은 다양한 프롬프트를 입력받아, 감지, 분할을 위한 지역화 토큰이 포함된 구조화된 텍스트를 출력할 수 있습니다.

지난해 [PaliGemma](https://huggingface.co/blog/paligemma)는 이러한 과제 해결을 시도한 최초의 모델이었습니다. 이 모델은 이미지와 텍스트(관심 객체에 대한 설명) 및 작업 접수사를 입력으로 받습니다. 텍스트 프롬프트는 "줄무늬 고양이를 분할하세요(segment cat behind)" 또는 "지붕 위의 새를 감지하세요(detect bird on the roof)"와 같습니다.

감지의 경우, 모델은 경계 상자 좌표를 토큰으로 출력합니다. 반면 분할의 경우, 모델은 감지 토큰과 분할 토큰을 출력합니다. 이러한 분할 토큰은 모든 분할된 픽셀 좌표가 아니라, 이러한 토큰을 유효한 분할 마스크로 디코딩하도록 훈련된 변분 오토인코더(VAE)에 의해 디코딩되는 코드북 인덱스입니다(아래 그림 참조).

![](https://i.imgur.com/c8tC5pd.png)


PaliGemma 이후 많은 모델들이 지역화 작업을 수행하기 위해 도입되었습니다. 작년 말, PaliGemma의 업그레이드 버전인 PaliGemma 2가 동일한 기능과 더 나은 성능으로 등장했습니다. 나중에 나온 또 다른 모델은 `Allen AI`의 Molmo로, 점으로 인스턴스를 가리키고 객체 인스턴스를 계수할 수 있습니다.

![](https://i.imgur.com/OT4oTEb.png)

Qwen2.5-VL도 객체를 감지하고, 가리키고, 계수할 수 있으며, 이는 UI 요소도 객체로 포함합니다!

![](https://i.imgur.com/63hKnua.png)

### 멀티모달 안전 모델

 프로덕션의 VLM은 탈옥과 규정 준수를 위한 유해한 출력을 방지하기 위해 입력과 출력을 필터링해야 합니다. 유해한 콘텐츠는 폭력적 입력부터 성적으로 노골적인 콘텐츠까지 다양합니다. 바로 여기에 멀티모달 안전 모델이 활용됩니다: 이들은 VLM의 입력과 출력을 필터링하기 위해 모델 전후에 배치됩니다. 이는 LLM 안전 모델과 유사하지만 추가 이미지 입력을 처리할 수 있습니다.

2025년 초, `Google`은 최초의 오픈 멀티모달 안전 모델인 [ShieldGemma 2](https://huggingface.co/google/shieldgemma-2-4b-it)를 소개했습니다. 이 모델은 텍스트 전용 안전 모델인 ShieldGemma를 기반으로 구축되었습니다. 이 모델은 이미지와 콘텐츠 정책을 입력받아 주어진 정책에 대해 이미지가 안전한지 여부를 반환합니다. 정책은 이미지가 부적절한 기준을 말합니다. ShieldGemma 2는 이미지 생성 모델의 출력을 필터링하는 데에도 사용될 수 있습니다.

`Meta`의 [Llama Guard 4](https://huggingface.co/spaces/merve/llama-guard-4)는 밀집 멀티모달 및 다국어 안전 모델입니다. 안전성 미세 조정과 함께 Llama 4 Scout(멀티모달 전문가 혼합 모델)에서 밀집하게 가지치기되었습니다.

![](https://i.imgur.com/CZdcvHf.png)

이 모델은 텍스트 전용 및 멀티모달 추론에 사용될 수 있습니다. 또한 VLM의 출력을 입력받아 대화를 완성하고, 사용자에게 전송하기 전에 필터링할 수 있습니다.
### 멀티모달 RAG: Retrievers, Rerankers
 이제 멀티모달 공간에서 검색-증강 생성(RAG)이 어떻게 진화했는지 살펴봅시다. 일반적으로 PDF로 된 복잡한 문서에 대한 RAG는 세 단계로 처리됩니다:
1. 문서를 완전히 텍스트로 파싱
2. 일반 텍스트와 쿼리를 검색기(Retriever)와 재정렬기(Reranker)에 전달하여 가장 관련성 있는 문서 얻기
3. 관련 컨텍스트와 쿼리를 LLM에 전달
기존 PDF 파서는 문서의 구조와 시각적 요소(레이아웃, 표, 이미지, 차트 등)를 보존하기 위해 여러 구성 요소로 이루어져 있으며, 이 모든 요소가 마크다운으로 변환됩니다. 하지만 이 설정은 유지 관리가 어려울 수 있습니다.

![](https://i.imgur.com/ysyxqdg.png)

그러나 VLM의 등장과 함께 이 문제가 해결되었습니다: 이제 멀티모달 검색기와 리랭커가 있습니다.

![](https://i.imgur.com/EDpIFVL.png)

멀티모달 검색기는 PDF 문서 스택과 쿼리를 입력으로 받아 신뢰도 점수와 함께 가장 관련성 있는 페이지 번호를 반환합니다. 점수는 페이지가 쿼리에 대한 답변을 포함할 가능성 또는 쿼리가 페이지와 얼마나 관련이 있는지를 나타냅니다. 이것은 취약한 파싱 단계를 우회합니다.

가장 관련성 있는 페이지는 쿼리와 함께 VLM에 공급되고, VLM이 답변을 생성합니다.

주요 멀티모달 리트리버 아키텍처는 두 가지입니다:
- 문서 스크린샷 임베딩(DSE, MCDSE)
- ColBERT 계열 모델(ColPali, ColQwen2, ColSmolVLM)

DSE 모델은 텍스트 인코더와 이미지 인코더로 구성되어 쿼리당 단일 벡터를 반환합니다. 반환된 점수는 임베딩의 내적에 대한 소프트맥스입니다. 구절당 단일 벡터를 반환합니다.

> 소프트맥스?
> 소프트맥스는 DSE 모델이 계산한 여러 문장의 '유사도 점수'를 '정답일 확률'로 변환하여, 모델이 가장 가능성 높은 선택지를 고르거나 학습할 수 있도록 돕는 핵심적인 장치입니다.

![](https://i.imgur.com/7oWZXCp.png)

ColPali와 같은 ColBERT류 모델도 이중 인코더 모델이지만 한 가지 차이점이 있습니다: ColPali는 이미지 인코더로 VLM을, 텍스트 인코더로 LLM을 가지고 있습니다. 이러한 모델은 본질적으로 인코더가 아니지만, 모델이 임베딩을 출력하고, 이것이 "MaxSim"으로 전달됩니다. 출력은 DSE와 달리 각 토큰당 하나씩 총 여러 개의 벡터로 이루어집니다. MaxSim에서 각 텍스트 토큰 임베딩과 각 이미지 패치 임베딩 간의 유사성이 계산되며, 이 접근 방식은 뉘앙스를 더 잘 포착합니다. 이러한 이유로 ColBERT류 모델은 비용 효율성이 낮지만 성능이 더 좋습니다.

아래는 ColPali의 인덱싱 지연 시간을 볼 수 있습니다. 단일 모델이기 때문에 유지 관리도 더 쉽습니다.

![](https://i.imgur.com/nllvvh5.png)


Hugging Face Hub에서 이러한 모델들은 "[Visual Document Retrieval](https://huggingface.co/models?pipeline_tag=visual-document-retrieval&sort=trending)" 작업 아래에서 찾을 수 있습니다.

이 작업에 가장 인기 있는 벤치마크는 ViDoRe로, 재무 보고서, 과학 그림부터 행정 문서까지 다양한 **영어** 및 **프랑스어** 문서로 구성되어 있습니다. ViDoRe의 각 예제에는 문서 이미지, 쿼리 및 잠재적 답변이 있습니다. 쿼리와 일치하는 문서는 대조 사전 훈련에 도움이 되므로 ViDoRe 훈련 세트는 새로운 모델을 훈련하는 데 사용됩니다.

## 멀티모달 에이전트
 VLM은 문서를 이용한 대화부터 컴퓨터 사용까지 많은 에이전트 워크플로우를 가능하게 합니다. 여기서는 더 고급 에이전트 기능이 필요하기 때문에 후자를 다룰 것입니다. 최근 UI를 이해하고 조작하는 VLM이 다수 출시되었습니다. 최신 모델은 `ByteDance`의 UI-TARS-1.5로, 브라우저, 컴퓨터 및 휴대폰 사용에서 훌륭한 결과를 보여주었습니다. 추론과 함께 게임플레이도 할 수 있으며 오픈 월드 게임 내 조작 가능합니다. 올해의 또 다른 주목할 만한 모델은 MAGMA-8B로, UI 탐색과 현실 세계의 물리적 상호작용 모두를 지원하는 파운데이션 모델입니다. 또한 Qwen2.5-VL(특히 에이전트 작업에 대해 추가 훈련된 32B 변형)과 Kimi-VL 추론 모델은 GUI 에이전트 작업에 탁월합니다.

2025년 초, 우리는 ReAct 프레임워크를 구현하는 새로운 경량 에이전트 라이브러리인 smolagents를 소개했습니다.(smolagents는 블로그의 [다른 포스트](https://hugging-face-krew.github.io/Introducing-smolagents/)에서도 번역 작업이 이루어졌습니다!) 그 직후, 라이브러리에 VLM 지원을 구현했습니다. 이 통합은 두 가지 사용 사례에서 이루어졌습니다:

- 실행 시작 시 한 번만 이미지 제공. 이것은 도구 사용이 포함된 문서 AI에 유용합니다.
- 동적으로 이미지 검색. VLM 에이전트를 통한 GUI 제어와 같이 에이전트가 반복적으로 스크린샷을 찍는 경우에 유용합니다.

라이브러리는 사용자가 이미지 이해 기능을 탑재한 자신만의 에이전트 워크플로우를 구축할 수 있는 빌딩 블록을 제공합니다. 우리는 사용자가 쉽게 시작할 수 있도록 다양한 스크립트와 한 줄로 된 CLI 명령을 제공합니다.

첫 번째 사례에서는 문서를 설명하는 에이전트가 필요하다고 가정합니다(에이전트적 특성은 약하지만 최소한의 사용 사례에는 좋습니다). 다음과 같이 CodeAgent(자체 코드를 작성하는 에이전트)를 초기화할 수 있습니다:

```python
agent = CodeAgent(tools=[], model=model) # 도구 불필요
agent.run("Describe these documents:", images=[document_1, document_2, document_3])
```

스크린샷을 가져와야 하는 후자의 사용 사례의 경우, 각 ActionStep 끝에 실행될 콜백을 정의할 수 있습니다. 동적으로 이미지를 가져와야 하는 여러분의 사용 사례에 맞게 콜백을 자유롭게 수정하세요. 간결함을 위해 여기서 자세히는 정의하지 않겠습니다. 선택적으로, 이 블로그 포스트 끝의 블로그 포스트와 스크립트 자체를 참고할 수 있습니다. 지금은 콜백과 브라우저 제어 단계를 통해 에이전트를 초기화하는 방법을 봅시다:

```python
def save_screenshot(memory_step: ActionStep, agent: CodeAgent) -> None:
    """
    스크린샷을 찍고 관찰에 씁니다.
    """
    png_bytes = driver.get_screenshot_as_png()
    memory_step.observations_images = [image.copy()] # memory_step에 이미지 유지
    url_info = f"Current url: {driver.current_url}"
    memory_step.observations = (
        url_info if memory_step.observations is None else memory_step.observations + "\n" + url_info
    )
    return

agent = CodeAgent(
    tools=[go_back, close_popups, search_item_ctrl_f], # 탐색 도구 전달
    model=model,
    additional_authorized_imports=["helium"],
    step_callbacks=[save_screenshot], # 콜백 전달
)
```

다음 CLI 명령을 실행하여 전체 예제를 간단히 시도해 볼 수 있습니다. 이 명령어는 웹 자동화 작업을 수행하기 위해 VLM로 구동되는 웹 브라우저에 대한 제어 권한이 있는 에이전트를 시작합니다(원하는 웹사이트로 교체하세요):

```shell
webagent "go to xyz.com/men, get to sale section, click the first clothing item you see. Get the product details, and the price, return them. note that I'm shopping from France"
```

smolagents는 로컬 트랜스포머 모델, 추론 제공자를 사용하여 제공되는 오픈소스 모델 또는 엔드포인트 폐쇄 소스 모델 제공자와 같은 다양한 모델 유형을 제공합니다. 현재 많은 에이전트 워크플로우가 추론을 필요로 하기 때문에 오픈소스 모델의 사용을 권장하며, 이는 많은 수의 파라미터를 가진 모델의 이점을 받습니다. 2025년 4월 기준으로 Qwen 2.5 VL은 에이전트 작업에 대해 추가로 훈련되었기 때문에 에이전트 워크플로우에 좋은 후보입니다.

## 비디오 언어 모델
 요즘 대부분의 VLM은 비디오를 처리할 수 있는데, 비디오는 프레임 시퀀스로 표현될 수 있기 때문입니다. 그러나 프레임 간의 시간적 관계와 많은 양의 프레임 때문에 비디오 이해가 까다로우므로, 대표성이 있는 비디오 프레임 집합을 선택하기 위해 다양한 기법이 사용됩니다.

![](https://i.imgur.com/4LKdfx6.png)

작년부터 커뮤니티는 이 문제를 해결하기 위한 다양한 접근 방식과 트릭에 무게를 두었습니다.

`Meta`의 [LongVU](https://huggingface.co/collections/Vision-CAIR/longvu) 모델이 좋은 예시입니다. 이 모델은 DINOv2에 비디오 프레임을 통과시켜 가장 유사한 프레임을 선택하여 제거함으로써 비디오 프레임을 다운샘플링하고, 그런 다음 텍스트 쿼리에 따라 가장 관련성 있는 프레임을 추가로 선별하여 프레임을 더욱 정제하는데, 여기서 텍스트와 프레임 모두 동일한 공간에 투영되고 유사성이 계산됩니다. [Qwen2.5VL](https://huggingface.co/collections/Qwen/qwen25-vl)은 긴 컨텍스트를 처리할 수 있고 모델이 다른 프레임 속도의 비디오로 훈련되기 때문에 동적 FPS 속도에 적응됩니다. 확장된 멀티모달 RoPE를 통해 프레임의 절대 시간 위치를 이해하고, 서로 다른 속도를 처리할 수 있으며 실제 생활에서 발생하는 사건의 속도를 여전히 이해할 수 있습니다. 또 다른 모델인 [Gemma 3](https://huggingface.co/collections/google/gemma-3-release)은 텍스트 프롬프트에 타임스탬프가 삽입된 비디오 프레임을 수용할 수 있습니다(예: “Frame 00.00: \<image>..”). 이 모델은 비디오 이해 작업에서 매우 우수한 성능을 보입니다.

![](https://i.imgur.com/Lr0A4Vt.png)


## VLM을 위한 새로운 정렬(alignment) 기법

 **선호도 최적화**는 VLM으로도 확장될 수 있는, 언어 모델을 위한 대체 미세 조정 접근법입니다. 고정된 레이블에 의존하는 대신, 이 방법은 선호도에 기반하여 후보 응답을 비교하고 순위를 매기는 데 중점을 둡니다. [trl](https://huggingface.co/docs/trl/en/index) 라이브러리는 VLM을 포함한 직접 선호도 최적화(DPO)를 지원합니다.

아래는 VLM 미세 조정의 DPO를 위한 선호도 데이터셋의 구조 예입니다. 각 항목은 이미지 + 질문 쌍과 두 개의 대응 답변(선택된 답변과 거부된 답변)으로 구성됩니다. VLM은 선호되는(선택된) 답변과 정렬된 응답을 생성하도록 미세 조정됩니다.

![](https://i.imgur.com/JniGe5G.png)

이 절차를 위한 예제 데이터셋은 [RLAIF-V](https://huggingface.co/datasets/openbmb/RLAIF-V-Dataset)로, 위에서 설명한 구조에 따라 포맷된 83000개 이상의 주석이 달린 샘플을 포함합니다. 각 항목에는 이미지 목록(보통 하나), 프롬프트, 선택된 답변, 거부된 답변이 포함되어 있으며, 이는 DPOTrainer가 예상하는 대로입니다.

이미 해당 형식으로 적절하게 포맷된 [RLAIF-V 포맷 데이터셋](https://huggingface.co/datasets/HuggingFaceH4/rlaif-v_formatted)이 있습니다. 아래는 단일 샘플의 예입니다:

```python
{'images': [<PIL.JpegImagePlugin.JpegImageFile image mode=L size=980x812 at 0x154505570>],
 'prompt': [ { "content": [ { "text": null, "type": "image" }, { "text": "What should this catcher be using?", "type": "text" } ], "role": "user" } ],
 'rejected': [ { "content": [ { "text": "The catcher, identified by the number...", "type": "text" } ], "role": "assistant" } ],
 'chosen': [ { "content": [ { "text": "The catcher in the image should be using a baseball glove...", "type": "text" } ], "role": "assistant" } ]}
```

데이터셋이 준비되면 trl 라이브러리의 DPOConfig 및 DPOTrainer 클래스를 사용하여 미세 조정 프로세스를 구성하고 시작할 수 있습니다.

아래는 DPOConfig를 사용한 예제 구성입니다:
```python
from trl import DPOConfig

training_args = DPOConfig(
    output_dir="smolvlm-instruct-trl-dpo-rlaif-v",
    bf16=True,
    gradient_checkpointing=True,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=32,
    num_train_epochs=5,
    dataset_num_proc=8, # 토큰화는 8개 프로세스 사용
    dataloader_num_workers=8, # 데이터 로딩은 8개 워커 사용
    logging_steps=10,
    report_to="tensorboard",
    push_to_hub=True,
    save_strategy="steps",
    save_steps=10,
    save_total_limit=1,
    eval_steps=10, # 평가를 위한 단계 간격
    eval_strategy="steps",
)
```

DPOTrainer를 사용하여 모델을 훈련하려면 보상 차이를 계산하기 위해 참조 모델을 선택적으로 제공할 수 있습니다. 파라미터 효율적 미세 조정(PEFT)을 사용하는 경우 `ref_model=None`을 설정하여 참조 모델을 생략할 수 있습니다:
```python
from trl import DPOTrainer

trainer = DPOTrainer(
    model=model,
    ref_model=None,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    peft_config=peft_config,
    tokenizer=processor
)
trainer.train()
```

## 새로운 벤치마크
 벤치마크 역시 지난 1년간 크게 발전했습니다. 이전 블로그에서 우리는 VLM을 평가하기 위한 두 가지 신흥 벤치마크로 MMMU와 MMBench를 소개했습니다. 해당 분야의 빠른 진전으로 모델들이 이러한 벤치마크에서 포화되었고, 더 나은 평가 도구가 필요합니다. 이를 달성하기 위해서는 범용 벤치마크 위에 특정 기능을 평가하는 도구가 필요합니다.

![](https://i.imgur.com/YaT28j7.png)

이제 두 가지 두드러진 범용 벤치마크인 MMT-Bench와 MMMU-Pro를 소개합니다.
### MMT-Bench
 MMT-Bench는 전문 지식, 정확한 시각적 인식, 지역화, 추론 및 계획이 필요한 광범위한 멀티모달 작업에 걸쳐 VLM을 평가하도록 설계되었습니다. 벤치마크는 이미지, 텍스트, 비디오 및 포인트 클라우드 모달리티의 다양한 멀티모달 시나리오에서 추출한 31325개의 객관식 시각 질문을 포함합니다. 162개의 하위 태스크가 있는 32개의 다른 메타 태스크로 OCR, 시각적 인식 또는 시각-언어 검색을 포함한 다양한 작업을 다룹니다.

### MMMU-Pro
 MMMU-Pro는 원래 MMMU 벤치마크의 더 나은 버전입니다. 여러 모달리티에 걸쳐 고급 AI 모델의 진정한 이해 능력을 평가합니다.
MMMU보다 더 복잡한 구조를 가지며, 예를 들어 비전 전용 입력 설정이 있고 후보 옵션 수가 4개에서 10개로 증가했습니다. 이 벤치마크는 실제 환경 시뮬레이션을 통합하여, 시뮬레이션된 디스플레이 내에서 캡처된 스크린샷이나 사진에서 파생된 비전 전용 문제를 포함합니다. 다양한 배경, 글꼴 스타일 및 크기를 적용하여 실제 환경 조건을 모방합니다.

## 추가 : 모델 추천Pick
주목할 만한 모델 몇 가지를 소개합니다. 선호하는 모델은 많지만, 아래는 최신 버전(2025년 5월 기준)입니다.

| 모델명                      | 파라미터                                          | 왜 추천하나요?                            |
| ------------------------ | --------------------------------------------- | ----------------------------------- |
| Qwen2.5-VL               | from 3B to 72B                                | 에이전트 기능, 수학 등 다양한 기능을 갖춘 뛰어난 다목적 모델 |
| RolmOCR                  | 7B                                            | 매우 성능이 우수한 OCR 모델                   |
| Kimi-VL-Thinking         | 16B MoE with 3B active parameters             | 가장 추론 능력이 우수한 모델                    |
| SmolVLM2                 | 256M, 500M (our favorite!), 2.2B              | 가장 작은 VLM                           |
| Llama 4 Scout & Maverick | 109B/400B MoE with 17B active parameters      | 너어어어어무 긴 컨텍스트                       |
| Molmo                    | 1B, 7B, 72B and MoE with 1B active parameters | 완전 개방형 모델에 지역화기능 추가됨                |

여기까지입니다! 아래에는 위 포스트의 각 주제에 대한 보다 심층적인 설명을 제공하는 링크입니다 :)

## 읽어볼만한 글
- [Models, datasets and more mentioned in this blog](https://huggingface.co/collections/sergiopaniego/vision-language-models-2025-update-682206d8ed0728be05dbf901)
- Multimodal Safety: [Llama Guard 4 Blog](https://huggingface.co/blog/llama-guard-4)
- DPO in VLMs: [Preference Optimization for Vision Language Models with TRL](https://huggingface.co/blog/dpo_vlm)
- Smolagents with VLM support: [We just gave sight to smolagents](https://huggingface.co/blog/smolagents-can-see)
- Agents Course section for Vision Agents using smolagents: [Vision Agents with smolagents](https://huggingface.co/learn/agents-course/unit2/smolagents/vision_agents)
- Gemma 3 Model Release: [Welcome Gemma 3: Google's all new multimodal, multilingual, long context open LLM](https://huggingface.co/blog/gemma3)
- PaliGemma 2 Model Release: [Welcome PaliGemma 2 – New vision language models by Google](https://huggingface.co/blog/paligemma2)
- [Pi0 release by Hugging Face](https://huggingface.co/blog/pi0)
- Multimodal retrieval: [Visually Multilingual: Introducing mcdse-2b](https://huggingface.co/blog/marco/announcing-mcdse-2b-v1)
- Multimodal retrieval: [ColPali: Efficient Document Retrieval with Vision Language Models](https://huggingface.co/blog/manu/colpali)
- Video Language Modelling: [SmolVLM2: Bringing Video Understanding to Every Device](https://huggingface.co/blog/smolvlm2)
- Minimal training of VLM with vanilla PyTorch: [GitHub - huggingface/nanoVLM: The simplest, fastest repository for training/finetuning small-sized VLMs.](https://github.com/huggingface/nanoVLM)